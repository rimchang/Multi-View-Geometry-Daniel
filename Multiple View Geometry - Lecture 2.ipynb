{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/001.png)\n",
    "이제 eigenvalue, eigen vector에 대해서 알아보자. 여기서 중요한 것은 A라는 매트릭스가 space를 어떻게 바꾸냐는 것이 중요하다. 즉 A라는 매트릭스는 v라는 축에대해서 스케일만을 수행하는 매트릭스라는 것이다.\n",
    "\n",
    "보통 정의가 된 것은 right eigenvector를 말하며 비슷하게 <b>left eigenvector</b>를 정의할 수 있다. set of eigenvector를 매트릭스 A의 <b>spectrum</b> 이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/002.png)\n",
    "\n",
    "1) right eigen vector가 존재하면 left eigen vector도 존재한다. 또한 A와 A^T의 set of eigenvector는 같다.   \n",
    "2) 다른 eigen value를 가지는 eigen vector끼리는 다 일차독립이다. (eigen vector 계산할때.. 같은 eigen value를 가지면.. solution 다 체크해줬던걸 기억해보자)  \n",
    "3) eigen value는 특성 방정식의 근이 된다. 즉.. det(A)는 모든 eigen value의 곱이 된다.   \n",
    "\n",
    "$Av=\\lambda v$  \n",
    "$=>(A-\\lambda I)v=0$\n",
    "\n",
    "$if\\,\\, det(A-\\lambda I) != 0$  \n",
    "$then \\,\\, v=0$\n",
    "\n",
    "eigen vector은 영벡터이면 안된다... 그러므로 det(A-\\lambda I) =0 중에서 solution을 찾게 된다.\n",
    "\n",
    "4) B=PAP^-1 일때 B와 A의 eigen vector는 같다  \n",
    "5) A의 eigen vector와 cunjugate eigen vector는 같다. (특성방정식을 풀때 복소수 공간에서는 항상 n개의 솔루션이 존재한다.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/003.png)\n",
    "symmetic 매트릭스의 eigenvector들은 아주 좋은 특징들을 가지고 있다. 이들의 spectrum은 매우 좋은 특징들을 가지고 있고 대각행렬은 실제에서도 아주 많이 나타난다. 공분산 행렬같은 경우 대각행렬이다.\n",
    "\n",
    "nxn 대각행렬은 다음과 같은 특징들을 가지고 있다.\n",
    "\n",
    "1. 모든 eigen value들이 실수이다.\n",
    "2. eigen value가 다른 eigen vector들은 직교한다.\n",
    "3. 대각행렬은 직교대각화(Orthogonal Diagonalize) 할수 있다. 이 의미는 orhonormal eigenvector를 구할 수 있다는 뜻이다.  \n",
    "4. S가 positive (semi) definete라면 모든 eigen value가 positive이다. (이 의미는 S가 non-negative function이라는 것을 뜻한다)  \n",
    "5. S가 positive semi definite라면 |x|=1 즉.. unit sphere를 이루는 벡터와의 $x^TSx$ 중의 최대값이 가장 큰 eigenvalue이며, 최소값이 가장 작은 고유값이다. 이러한 이유는 eigenvalue는 어떤 방향의 고유벡터에 대해 scale을 해주는 값이다. unit sphere 즉 모든 방향의 벡터에 대해 S라는 선형사상을 매핑해보고 그중 가장 큰 값이 가장 큰 eigen value가된다.\n",
    "\n",
    "pf) 2. eigen value가 다른 eigen vector들은 직교한다.\n",
    "\n",
    "\n",
    "$v_i^TSv_j = v_i^T \\lambda_j v_j $ (v_j가 고유벡터이므로)  \n",
    "$v_i^TSv_j = v_i^T\\lambda_i v_j $ (v_i가 고유벡터이므로)  \n",
    "\n",
    "=>$(\\lambda _i - \\lambda _j) v_i^Tv_j = 0$\n",
    "\n",
    "즉 위의 수식의 의미는 if $(\\lambda _i - \\lambda _j) != 0$ 즉 람다i,j가 같지 않다면 영벡터를 만들기 위해 v_i, v_j 내적이 0 이여야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/004.png)\n",
    "\n",
    "<b>induced 2-norm</b> : 모든 가능한 unit vector에 선형사상을 취한 vector의 norm의 max값으로 정의한다. (이는 max|lambda| 와 매우 비슷하지 않나? 라는...나의생각 맞는것 같다 max(|lambda|) for all labmda 가 된다.) ,  \n",
    "\n",
    "오른쪽에 유도된 것은 일단 |Ax| 는 Ax라는 벡터의 2 nomr이다. Ax는 벡터이며 벡터의 norm은 내적, Ax랑 Ax를 내적하면 $x^TA^TAx = <x, A^TAx>$ 이다.\n",
    "\n",
    "<b>Frobenius norm</b> : 매트릭스의 모든 power of element를 더하고 이에 제곱근이다. 이를 매트릭스로 표현하면 trace(A^TA) 라고한다...(고유값들의 제곱합의 제곱근이다.)\n",
    "\n",
    "위의 두 norm은 다르며 쓰임새에 따라 다르게 적용된다. 밑의 예제가 induced 2-norm과 frobenius norm이 다른 경우를 보여준다. $x^TA^TAx$ >= 0 for all v 가 semi definite의 정의에 따라 만족해야하는데.. $x^TA^TAx$ 은 |Ax| 이다. 즉 Ax의 norm이며 norm은 non-negative이다.\n",
    "\n",
    "indced l2-norm은 정의에 따라 절대값이 가장 큰 고유값이며, frobenius norm은 A^TA 매트릭스의 고유값들의 제곱합이된다. (trace(A)=sum of eigen value 라는 사실을 기억해보자)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/005.png)\n",
    "\n",
    "<b>skew-symmetric</b> 매트릭스의 트랜스포즈가 negative scale 매트릭스면 skew-symmetric이라고 한다. (diagnal이 항상 0이여야 한다. -scale이여도 변하지 않으려면 0이여만 한다.)\n",
    "\n",
    "1. skew-symmetric matrix는 고유값이 0 혹은 허수이다. \n",
    "2. block-diagonal matrix 대각화를 할 수 있다.  \n",
    "\n",
    "$\\begin{pmatrix}\n",
    "x &y \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    " 0& a_i\\\\-a_i \n",
    " & 0\n",
    "\\end{pmatrix}\\binom{x}{y} = \\binom{a_ixy}{-a_ixy}$ 즉 일종의 skew된.. space를 만든다는 것 같다.\n",
    "\n",
    "이는 eigen value가 2x2 block의 요소 or 0 이므로 rank가 짝수이다,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/006.png)\n",
    "3차원 실수공간에서의 vector u=(u1,u2,u3) 이 있다고 했을때 이의 <b>hat operator</b> 는 위와 같이 정의된다.  이 연산이 하는 일은 R^3의 공간의 벡터를 R^3x3 의 공간으로 매핑해준다. \n",
    "\n",
    "이는 mimic of cross product로 사용된다. 즉 cross product를 매트릭스연산의 형태로 계산이 가능해진다. 위에서 skew symetric matrix의 랭크가 짝수이라는 것을 상기했을때 이 hat operator의 랭크는 2 혹은 0 이 된다. \n",
    "\n",
    "null_space를 구성하는 벡터는 u 자신이 되는데,, u^hat이 mimic of cross product인 것을 생각해보자. 즉 u와 u의 cross product는 0 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/007.png)\n",
    "svd는 매트릭스가 space에 무슨일을 하는지를 compact하게 나타내 준다. 이는 eigenvalue decomposition의 일반화된 버젼이라고 볼 수 있다. 고유값 분해는 nxn 매트릭스에만 정의되었다. 앞에서 봤듯이 A^TA 는 A가 어떤 매트릭스인지 상관없이 square matrix가 되며 symmetric(positive semi definite)이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/008.png)\n",
    "SVD는 eigenvalue decomposition의 일반화이며 U=V 인 경우에 eigenvalue decomposition과 동치가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/009.png)\n",
    "\n",
    "any matrix A가 주어졌을때 A^TA 는 symmetric, positive semi-definte이다. symmetric 매트릭스는 직교대각화가 가능하며 power of eigenvalue 순으로 정렬한다. (positive semi-definite 매트릭스의 eigen value는 모두 non-negative이다. 제곱해도 순서를 유지하는 것을 유념해라) \n",
    "\n",
    "이렇게 구한 eigen value의 제곱근을 singular value라고 한다. \n",
    "\n",
    "1. Note : $Ker(A^TA) = ker(A), \\,\\,\\,\\, range(A^TA) = range(A^T) $ (not trivial), A^TA의 고유벡터를 알고 있음으로 ker(A), range(A^TA) 이 무슨 space인지에 대한 정보를 알 수 있다.\n",
    "\n",
    "2. 벡터 u_i 를 위와 보이는 것 같이 정의하면 $Av_i = \\sigma_iu_i$ 를 얻을 수 있다. 이 의미는 다음과 같다. 고유벡터 분해에서의 $Av_i = \\lambda_i v_i$ 의 의미는 A라는 선형사상을 v_i에 적용하면 scale된 v_i를 얻을 수 있다. 라는 것이다 $Av_i = \\sigma_iu_i$ 의 의미는 A라는 선형사상을 v_i에 적용하면 scale된 u_i를 얻을 수 있다. \n",
    "\n",
    "3. u_i 들은 정규직교 벡터인데. u_i,u_j 의 내적을 계산해보면 v_i,v_j 간으 내적으로 표현되고 v_i, v_j는 직교대각화 해서 얻은 고유벡터임으로 정규직교벡터이다. delta_ij는 밑과 같다.\n",
    "\n",
    "$where \\,\\,\\, \\delta _{ij} = \\begin{cases}\n",
    "1 & \\text{ if } i= j\\\\ \n",
    "0 & \\text{ otherwise } \n",
    "\\end{cases} $\n",
    "\n",
    "pf) $<Av_i,Av_j>=0 \\,  or \\, 1$\n",
    "\n",
    "$<Av_i,Av_j> = v_i^TA^TAv_j = <v_i, A^TAv_j> = <v_i, \\sigma_j^2 v_j>  = 0 \\,\\,\\, where\\,\\, i!=j\n",
    "$ (v_j가 A^TA의 정규벡터인것과, v_i, v_j 가 정규직교벡터이므로)  \n",
    "i=j일 때는 분모분자가 같아져 1이된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/010.png)\n",
    "p개의 u_i를 m개로 맞추기 위해 영벡터를 껴주고 매트릭스로 나타내주면 된다. 그다음 0벡터가 곱해지는 부분들을 지워주면 증명완료!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/011.png)\n",
    "<b>The square matrix A maps the unit sphere into an ellipsoid with semi-axes sigma_i</b>A라는 매트릭스는 R^n의 point x를 R^n으로 매핑해주는 사상이며 U를 기저로하는 coordinate y는 V를 기저로하는 coordinate x와 $\\Sigma$ 만큼 관련되어져 있다. 중간의 오른쪽 식을 보자 앞의 강의에서 basis에서 다른 basis로의 transform을 해줄수 있다. $V^Tx$ 는.. 결국 range(V) 에서의 어떤 한 point가 되며 이 공간의 기저는 V이다. $U^Ty$ 는 range(U) 에서의 어떤 한 point이며 이 공간의 기저는 U이다. $\\Sigma$  이 하는 역할은 scaling뿐이며 즉 A라는 매트릭스는 span(V) 에서 span(U) 로 point를 매핑해주는 사상이며 span(U), span(V) 는 단지 scaling 만이 다른 공간이다. (그래서 unit sphere를 타원으로 만들어주는 연산??이라고 했나보다)\n",
    "\n",
    "which represents the equation of an ellipsoid with half-axes of length σ_i(반지름이 σ_i인 타원으로 매핑해준다고 한다) 정리를하자면.. A매트릭스는 x좌표를 V 기저로 매핑하고 스케일하는 사상이다. 그 스케일된 V 기저가.. 바로 U인듯.. 즉 일반적인 매트릭스가 하는 일은 어떤 축에 대한 스케일링..!! 이것이 결국 매트릭스가 하는일.. 이때 스케일링할때 반지름이 σ_i가 되도록 스케일링 해주는것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/012.png)\n",
    "SVD가 쓰이는 것중 하나가 linear equation을 풀때 사용된다. Ax=b 라는 수식을 linear equation을 풀기 위해서 운이 좋게 A가 square이며 invertible하다면... 역행렬을 통해 바로 구할 수 있다. 하지만 불행하게도 A가 non-invertible이거나 non-square이면 불가능하다. \n",
    "\n",
    "그냥.. SVD를 하고 이를 통해 pseudo inverse를 다음과 같이 정의한다. 저 수식의 의미는 inversion을 수행하는데 오직 non-zero singluar value에만 insion을 수행한다. 즉 non-zero singular value를 가지는 sub-space에 대한 역사상을 구하는 것이다. zero singluar value인 space는 결국 영향을 주지 않게 된다. 대략적으로 말해서는 A라는 매트릭스를 최대한 가능한 만큼(non-zero singular value of sub-space) 이라도 invert를 하는 것이다. \n",
    "\n",
    "이 pseudo inverse는 inverse matrix와 비슷한 연산을 하게 된다.\n",
    "\n",
    "$AA^\\dagger A = A$ 의 의미는 $A^-1A=I$ 이듯이 $AA^\\dagger$ 가 A에 영향을 주지 않는다는 것(항등원)이며 오른쪽 수식은 $A^\\dagger A$ 가 $A^\\dagger$ 에 항등원처럼 작동한다는 것이다. Ax=b 에서 여러개의 솔루션이 존재 할때 어떠한 솔루션을 선택하는지는 practically 하게 매우 중요하다.    <b>$A^\\dagger b$ 은 $|Ax-b|^2$ 를 minimize하는 one of smallest norm solution을 구하게 된다고 한다!!</b> 이는 매우 재미있는 것인데.. 만약 솔루션이 존재하지 않는다면 가능한 솔루션에 가까운 답을 내놓으려고 하며 수많은 솔루션이 존재한다면 그중 smallest norm을 구해준다는 것이다, \n",
    "\n",
    "why smallest norm이 좋은가?? generalization으로 설명할 수도 있고.. 하나를 생각해보자 x,y,z 라는 상품이 있고 a,b,c만큼 생산을 함으로써 outcome을 최대로 하고 싶다고 해보자..이를 linear equation을 풀었는데 여러개의 솔루션이 나옴. 이경우 같은 outcome을 얻더라도 생산(coeficient)을 최소로 하는 것이 더 좋다.. 이는 일종의 제약조건으로 작동한다. (오... 꽤 재밌는 예제인듯 싶다)\n",
    "\n",
    "어떻게 smallest norm을 구해주는 것인가?? invertible한 sub-space만을 invert하게 만들었고 나머지 space는 다 0으로 놨둔것이 pseudo inverse,, 그니까 pseudo inverse로 구해지는 x_min 들은 non-zero singluar value 부분만 값이 있고 나머지는 다 0이 되버릴 것이다. (일종의 l1-norm 정규화야??,,, 둘간의 관계가 있을라나?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/013.png)\n",
    "하나의 카메라가 객체의 주의를 돌면서 찍을때.. moving scene을 구하는 것을 공부할 것이다. 특히 이강의에서는 두개의 이미지를 통해 뭔가 하는 것을 중점적으로 배울 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/015.png)\n",
    "3d reconstruction의 목적은 two or multi view image가 주어졌을때.. 또한 scence이 변하지 않는다고 가정했을때(이 강의에서 주로 배울 것들은.. 이러한 가정..) 이는 ill-posed probelm인데 이는 사실..수학적으로 잘 정의된 개념이지만 직관적으로 설명하면 no unique solution 이라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./src/lecture2/016.png)\n",
    "1913년 kruppa는 5 point at two-view 만 있다면 camera motion을 계산가능하고 3d location을 계산하기 위한 finite solution을 얻을 수 있다는 것을 보였다.  \n",
    "\n",
    "여기서 말하는 stucture는 3d location of point in the world를 말하며 motion은 camera motion을 뜻한다. 컴퓨터비전에서는 이러한 문제를 보통 strcture and motion이라고 불렀고 robotics 쪽에서는 visual SLAM(simultanetous localization and mapping)이라고 불렀다고 한다. localization(motion과 비슷)이란 로봇 혹은 카메라가 어디에 위치하는지를 알아내는 것이고 mapping(structure와 비슷)은 3d point를 매핑하는 것이다. 보통 laser scan으로하는 SLAM이 있고 visual SLAM은 카메라를 통해 하는 SLAM이라고 한다!."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
